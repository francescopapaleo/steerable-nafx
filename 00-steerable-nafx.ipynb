{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNoMnmcyG0GC"
      },
      "source": [
        "<div align=\"center\">\n",
        "\n",
        "# Steerable discovery of neural audio effects\n",
        "\n",
        "  [Christian J. Steinmetz](https://www.christiansteinmetz.com/)  and  [Joshua D. Reiss](http://www.eecs.qmul.ac.uk/~josh/)\n",
        "\n",
        "\n",
        "[Code](https://github.com/csteinmetz1/steerable-nafx) • [Paper](https://arxiv.org/abs/2112.02926) • [Demo](https://csteinmetz1.github.io/steerable-nafx)\t• [Slides]()\n",
        "\n",
        "<img src=\"https://csteinmetz1.github.io/steerable-nafx/assets/steerable-headline.svg\">\n",
        "\n",
        "</div>\n",
        "\n",
        "## Abtract\n",
        "Applications of deep learning for audio effects often focus on modeling analog effects or learning to control effects to emulate a trained audio engineer. \n",
        "However, deep learning approaches also have the potential to expand creativity through neural audio effects that enable new sound transformations. \n",
        "While recent work demonstrated that neural networks with random weights produce compelling audio effects, control of these effects is limited and unintuitive.\n",
        "To address this, we introduce a method for the steerable discovery of neural audio effects.\n",
        "This method enables the design of effects using example recordings provided by the user. \n",
        "We demonstrate how this method produces an effect similar to the target effect, along with interesting inaccuracies, while also providing perceptually relevant controls.\n",
        "\n",
        "\n",
        "\\* *Accepted to NeurIPS 2021 Workshop on Machine Learning for Creativity and Design*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6x_gvWLIC8c"
      },
      "source": [
        "# Setup\n",
        "Run this first to install and import the relevant things..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HOkNOXFMles",
        "outputId": "013118d9-915f-4cca-8bce-96b780053008"
      },
      "outputs": [],
      "source": [
        "!pip install torchaudio auraloss pyloudnorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86Gcs-PCwqmr"
      },
      "source": [
        "Download some sounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osc0jS90rlsR",
        "outputId": "3e269a78-3f8f-41e0-ebe4-21ab205438de"
      },
      "outputs": [],
      "source": [
        "!wget https://csteinmetz1.github.io/sounds/assets/drum_kit_clean.wav \n",
        "!wget https://csteinmetz1.github.io/sounds/assets/drum_kit_comp_agg.wav \n",
        "!wget https://csteinmetz1.github.io/sounds/assets/acgtr_clean.wav \n",
        "!wget https://csteinmetz1.github.io/sounds/assets/acgtr_reverb.wav \n",
        "!wget https://csteinmetz1.github.io/sounds/assets/piano_clean.wav "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LQ0UXqVws8s"
      },
      "source": [
        "Download the pre-trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSr4zW34wszU",
        "outputId": "7d788e4a-89a2-495f-eeaa-51a2186d5688"
      },
      "outputs": [],
      "source": [
        "!wget https://csteinmetz1.github.io/steerable-nafx/models/compressor_full.pt > /dev/null\n",
        "!wget https://csteinmetz1.github.io/steerable-nafx/models/reverb_full.pt > /dev/null\n",
        "!wget https://csteinmetz1.github.io/steerable-nafx/models/amp_full.pt > /dev/null\n",
        "!wget https://csteinmetz1.github.io/steerable-nafx/models/delay_full.pt > /dev/null\n",
        "!wget https://csteinmetz1.github.io/steerable-nafx/models/synth2synth_full.pt > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHTl-3D-8Ar4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import math\n",
        "import torch\n",
        "import librosa.display\n",
        "import IPython\n",
        "import auraloss\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "import matplotlib\n",
        "import pyloudnorm as pyln\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsxRlrBnYQiw"
      },
      "outputs": [],
      "source": [
        "# Sources from:\n",
        "# https://github.com/LCAV/pyroomacoustics/blob/master/pyroomacoustics/experimental/rt60.py\n",
        "def measure_rt60(h, fs=1, decay_db=30, rt60_tgt=None):\n",
        "    \"\"\"\n",
        "    Analyze the RT60 of an impulse response.\n",
        "    Args:\n",
        "        h (ndarray): The discrete time impulse response as 1d array.\n",
        "        fs (float, optional): Sample rate of the impulse response. (Default: 48000)\n",
        "        decay_db (float, optional): The decay in decibels for which we actually estimate the time. (Default: 60)\n",
        "        rt60_tgt (float, optional): This parameter can be used to indicate a target RT60. (Default: None)\n",
        "    Returns:\n",
        "        est_rt60 (float): Estimated RT60.\n",
        "    \"\"\"\n",
        "\n",
        "    h = np.array(h)\n",
        "    fs = float(fs)\n",
        "\n",
        "    # The power of the impulse response in dB\n",
        "    power = h ** 2\n",
        "    energy = np.cumsum(power[::-1])[::-1]  # Integration according to Schroeder\n",
        "\n",
        "    try:\n",
        "        # remove the possibly all zero tail\n",
        "        i_nz = np.max(np.where(energy > 0)[0])\n",
        "        energy = energy[:i_nz]\n",
        "        energy_db = 10 * np.log10(energy)\n",
        "        energy_db -= energy_db[0]\n",
        "\n",
        "        # -5 dB headroom\n",
        "        i_5db = np.min(np.where(-5 - energy_db > 0)[0])\n",
        "        e_5db = energy_db[i_5db]\n",
        "        t_5db = i_5db / fs\n",
        "\n",
        "        # after decay\n",
        "        i_decay = np.min(np.where(-5 - decay_db - energy_db > 0)[0])\n",
        "        t_decay = i_decay / fs\n",
        "\n",
        "        # compute the decay time\n",
        "        decay_time = t_decay - t_5db\n",
        "        est_rt60 = (60 / decay_db) * decay_time\n",
        "    except:\n",
        "        est_rt60 = np.array(0.0)\n",
        "\n",
        "    return est_rt60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWjFxVtfMtq8"
      },
      "outputs": [],
      "source": [
        "def causal_crop(x, length: int):\n",
        "    if x.shape[-1] != length:\n",
        "        stop = x.shape[-1] - 1\n",
        "        start = stop - length\n",
        "        x = x[..., start:stop]\n",
        "    return x\n",
        "\n",
        "class FiLM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cond_dim,  # dim of conditioning input\n",
        "        num_features,  # dim of the conv channel\n",
        "        batch_norm=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.batch_norm = batch_norm\n",
        "        if batch_norm:\n",
        "            self.bn = torch.nn.BatchNorm1d(num_features, affine=False)\n",
        "        self.adaptor = torch.nn.Linear(cond_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "\n",
        "        cond = self.adaptor(cond)\n",
        "        g, b = torch.chunk(cond, 2, dim=-1)\n",
        "        g = g.permute(0, 2, 1)\n",
        "        b = b.permute(0, 2, 1)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            x = self.bn(x)  # apply BatchNorm without affine\n",
        "        x = (x * g) + b  # then apply conditional affine\n",
        "\n",
        "        return x\n",
        "\n",
        "class TCNBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, dilation, cond_dim=0, activation=True):\n",
        "    super().__init__()\n",
        "    self.conv = torch.nn.Conv1d(\n",
        "        in_channels, \n",
        "        out_channels, \n",
        "        kernel_size, \n",
        "        dilation=dilation, \n",
        "        padding=0, #((kernel_size-1)//2)*dilation,\n",
        "        bias=True)\n",
        "    if cond_dim > 0:\n",
        "      self.film = FiLM(cond_dim, out_channels, batch_norm=False)\n",
        "    if activation:\n",
        "      #self.act = torch.nn.Tanh()\n",
        "      self.act = torch.nn.PReLU()\n",
        "    self.res = torch.nn.Conv1d(in_channels, out_channels, 1, bias=False)\n",
        "\n",
        "  def forward(self, x, c=None):\n",
        "    x_in = x\n",
        "    x = self.conv(x)\n",
        "    if hasattr(self, \"film\"):\n",
        "      x = self.film(x, c)\n",
        "    if hasattr(self, \"act\"):\n",
        "      x = self.act(x)\n",
        "    x_res = causal_crop(self.res(x_in), x.shape[-1])\n",
        "    x = x + x_res\n",
        "\n",
        "    return x\n",
        "\n",
        "class TCN(torch.nn.Module):\n",
        "  def __init__(self, n_inputs=1, n_outputs=1, n_blocks=10, kernel_size=13, n_channels=64, dilation_growth=4, cond_dim=0):\n",
        "    super().__init__()\n",
        "    self.kernel_size = kernel_size\n",
        "    self.n_channels = n_channels\n",
        "    self.dilation_growth = dilation_growth\n",
        "    self.n_blocks = n_blocks\n",
        "    self.stack_size = n_blocks\n",
        "\n",
        "    self.blocks = torch.nn.ModuleList()\n",
        "    for n in range(n_blocks):\n",
        "      if n == 0:\n",
        "        in_ch = n_inputs\n",
        "        out_ch = n_channels\n",
        "        act = True\n",
        "      elif (n+1) == n_blocks:\n",
        "        in_ch = n_channels\n",
        "        out_ch = n_outputs\n",
        "        act = True\n",
        "      else:\n",
        "        in_ch = n_channels\n",
        "        out_ch = n_channels\n",
        "        act = True\n",
        "      \n",
        "      dilation = dilation_growth ** n\n",
        "      self.blocks.append(TCNBlock(in_ch, out_ch, kernel_size, dilation, cond_dim=cond_dim, activation=act))\n",
        "\n",
        "  def forward(self, x, c=None):\n",
        "    for block in self.blocks:\n",
        "      x = block(x, c)\n",
        "\n",
        "    return x\n",
        "  \n",
        "  def compute_receptive_field(self):\n",
        "    \"\"\"Compute the receptive field in samples.\"\"\"\n",
        "    rf = self.kernel_size\n",
        "    for n in range(1, self.n_blocks):\n",
        "        dilation = self.dilation_growth ** (n % self.stack_size)\n",
        "        rf = rf + ((self.kernel_size - 1) * dilation)\n",
        "    return rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vdTTil3ynQJ"
      },
      "outputs": [],
      "source": [
        "# setup the pre-trained models\n",
        "model_comp = torch.load(\"compressor_full.pt\", map_location=\"cpu\").eval()\n",
        "model_verb = torch.load(\"reverb_full.pt\", map_location=\"cpu\").eval()\n",
        "model_amp = torch.load(\"amp_full.pt\", map_location=\"cpu\").eval()\n",
        "model_delay = torch.load(\"delay_full.pt\", map_location=\"cpu\").eval()\n",
        "model_synth = torch.load(\"synth2synth_full.pt\", map_location=\"cpu\").eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qdmH_MbLSpq"
      },
      "source": [
        "# 1. Pre-trained models\n",
        "Jump right in by processing your own audio with some pre-trained models. \n",
        "\n",
        "1. Upload your input audio.\n",
        "2. Select your desired pre-trained model.\n",
        "3. Set the audio effect parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "Olfk0UJTuZ8O",
        "outputId": "bde5cecb-ac29-47c5-a189-1505b1e365d8"
      },
      "outputs": [],
      "source": [
        "#@title Upload input audio\n",
        "process_upload = files.upload()\n",
        "process_file = list(process_upload.keys())[-1]\n",
        "x_p, sample_rate = torchaudio.load(process_file)\n",
        "print(process_file, x_p.shape)\n",
        "IPython.display.display(IPython.display.Audio(data=x_p, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcOnTXLT8ZK2"
      },
      "source": [
        "Now set the audio effect parameters. \n",
        "Here are some more insights into the controls:\n",
        "\n",
        "- `effect_type` - Choose from one of the pre-trained models.\n",
        "- `gain_dB` - Adjust the input gain. This can have a big effect since the effects are very nonlinear.\n",
        "- `c0` and `c1` - These are the effect controls which will adjust perceptual aspects of the effect, depending on the effect type. Very large values will often result in more extreme effects.\n",
        "- `mix` - Control the wet/dry mix of the effect.\n",
        "- `width` - Increase stereo width of the effect.\n",
        "- `max_length` - If you uploaded a very long file this will truncate it.\n",
        "- `stereo` - Convert mono input to stereo output.\n",
        "- `tail` - If checked, we will also compute the effect tail (nice for reverbs). \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "Fgb6Kd7vtsFr",
        "outputId": "985e3bbb-311c-4156-ed11-bfee02a3b6a5"
      },
      "outputs": [],
      "source": [
        "effect_type = \"Compressor\" #@param [\"Compressor\", \"Reverb\", \"Amp\", \"Analog Delay\", \"Synth2Synth\"]\n",
        "gain_dB = -24 #@param {type:\"slider\", min:-24, max:24, step:0.1}\n",
        "c0 = -1.4 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "c1 = 3 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "mix = 70 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "width = 50 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "max_length = 30 #@param {type:\"slider\", min:5, max:120, step:1}\n",
        "stereo = True #@param {type:\"boolean\"}\n",
        "tail = True #@param {type:\"boolean\"}\n",
        "\n",
        "# select model type\n",
        "if effect_type == \"Compressor\":\n",
        "  pt_model = model_comp\n",
        "elif effect_type == \"Reverb\":\n",
        "  pt_model = model_verb\n",
        "elif effect_type == \"Amp\":\n",
        "  pt_model = model_amp\n",
        "elif effect_type == \"Analog Delay\":\n",
        "  pt_model = model_delay\n",
        "elif effect_type == \"Synth2Synth\":\n",
        "  pt_model = model_synth\n",
        "\n",
        "# measure the receptive field\n",
        "pt_model_rf = pt_model.compute_receptive_field()\n",
        "\n",
        "# crop input signal if needed\n",
        "max_samples = int(sample_rate * max_length)\n",
        "x_p_crop = x_p[:,:max_samples]\n",
        "chs = x_p_crop.shape[0]\n",
        "\n",
        "# if mono and stereo requested\n",
        "if chs == 1 and stereo:\n",
        "  x_p_crop = x_p_crop.repeat(2,1)\n",
        "  chs = 2\n",
        "\n",
        "# pad the input signal\n",
        "front_pad = pt_model_rf-1\n",
        "back_pad = 0 if not tail else front_pad\n",
        "x_p_pad = torch.nn.functional.pad(x_p_crop, (front_pad, back_pad))\n",
        "\n",
        "# design highpass filter\n",
        "sos = scipy.signal.butter(\n",
        "    8, \n",
        "    20.0, \n",
        "    fs=sample_rate, \n",
        "    output=\"sos\", \n",
        "    btype=\"highpass\"\n",
        ")\n",
        "\n",
        "# compute linear gain \n",
        "gain_ln = 10 ** (gain_dB / 20.0)\n",
        "\n",
        "# process audio with pre-trained model\n",
        "with torch.no_grad():\n",
        "  y_hat = torch.zeros(x_p_crop.shape[0], x_p_crop.shape[1] + back_pad)\n",
        "  for n in range(chs):\n",
        "    if n == 0:\n",
        "      factor = (width*5e-3)\n",
        "    elif n == 1:\n",
        "      factor = -(width*5e-3)\n",
        "    c = torch.tensor([float(c0+factor), float(c1+factor)]).view(1,1,-1)\n",
        "    y_hat_ch = pt_model(gain_ln * x_p_pad[n,:].view(1,1,-1), c)\n",
        "    y_hat_ch = scipy.signal.sosfilt(sos, y_hat_ch.view(-1).numpy())\n",
        "    y_hat_ch = torch.tensor(y_hat_ch)\n",
        "    y_hat[n,:] = y_hat_ch\n",
        "\n",
        "# pad the dry signal \n",
        "x_dry = torch.nn.functional.pad(x_p_crop, (0,back_pad))\n",
        "\n",
        "# normalize each first\n",
        "y_hat /= y_hat.abs().max()\n",
        "x_dry /= x_dry.abs().max()\n",
        "\n",
        "# mix\n",
        "mix = mix/100.0\n",
        "y_hat = (mix * y_hat) + ((1-mix) * x_dry)\n",
        "\n",
        "# remove transient\n",
        "y_hat = y_hat[...,8192:]\n",
        "y_hat /= y_hat.abs().max()\n",
        "\n",
        "torchaudio.save(\"output.mp3\", y_hat.view(chs,-1), sample_rate, compression=320.0)\n",
        "print(\"Done.\")\n",
        "print(\"Sending audio to browser...\")\n",
        "\n",
        "# show the audio\n",
        "IPython.display.display(IPython.display.Audio(\"output.mp3\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93paJZ591QQx"
      },
      "source": [
        "Click the three dots to download your processed audio file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPjhsA8krm0w"
      },
      "source": [
        "# 2. Steering (training)\n",
        "Use a pair of audio examples in order to construct neural audio effects.\n",
        "\n",
        "There are two options. Either start with the pre-loaded audio examples, or upload your own clean/processed audio recordings for the steering process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBbLuQpbsb97"
      },
      "source": [
        "a.) Use some of our pre-loaded audio examples. Choose from the compressor or reverb effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HCeohtbwseO3"
      },
      "outputs": [],
      "source": [
        "#@title Use pre-loaded audio examples for steering\n",
        "effect_type = \"Compressor\" #@param [\"Compressor\", \"Reverb\"]\n",
        "\n",
        "if effect_type == \"Compressor\":\n",
        "  input_file = \"drum_kit_clean.wav\"\n",
        "  output_file = \"drum_kit_comp_agg.wav\"\n",
        "elif effect_type == \"Reverb\":\n",
        "  input_file = \"acgtr_clean.wav\"\n",
        "  output_file = \"acgtr_reverb.wav\"\n",
        "\n",
        "x, sample_rate = torchaudio.load(input_file)\n",
        "x = x[0:1,:]\n",
        "\n",
        "y, sample_rate = torchaudio.load(output_file)\n",
        "y = y[0:1,:]\n",
        "\n",
        "print(\"input file\", x.shape)\n",
        "IPython.display.display(IPython.display.Audio(data=x, rate=sample_rate))\n",
        "print(\"output file\", y.shape)\n",
        "IPython.display.display(IPython.display.Audio(data=y, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53vG-PEDLdPx"
      },
      "source": [
        "  b.) or, load you own input/output sounds. \n",
        "  \n",
        "  The files must have the same length.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DzpK6h9uLc5Y"
      },
      "outputs": [],
      "source": [
        "#@title Upload clean sound (x)\n",
        "# upload the clean input file\n",
        "input_upload = files.upload()\n",
        "input_file = list(input_upload.keys())[-1]\n",
        "x, sample_rate = torchaudio.load(input_file)\n",
        "print(input_file, x.shape)\n",
        "IPython.display.display(IPython.display.Audio(data=x, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "99lY0c8ZLlPJ"
      },
      "outputs": [],
      "source": [
        "# upload the same file processed with an effect\n",
        "#@title Upload processed sound (y)\n",
        "output_upload = files.upload()\n",
        "output_file = list(output_upload.keys())[-1]\n",
        "y, sample_rate = torchaudio.load(output_file)\n",
        "\n",
        "if not y.shape[-1] == x.shape[-1]:\n",
        "  print(f\"Input and output files are different lengths! Found clean: {x.shape[-1]} processed: {y.shape[-1]}.\")\n",
        "  if y.shape[-1] > x.shape[-1]:\n",
        "    print(f\"Cropping target...\")\n",
        "    y = y[:,:x.shape[-1]]\n",
        "  else:\n",
        "    print(f\"Cropping input...\")\n",
        "    x = x[:,:y.shape[-1]]\n",
        "\n",
        "print(output_file, y.shape)\n",
        "IPython.display.display(IPython.display.Audio(data=y, rate=sample_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLulasUas996"
      },
      "source": [
        "Now its time to generate the neural audio effect by training the TCN to emulate the input/output function from the target audio effect. Adjusting the parameters will enable you to tweak the optimization process. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Z06ODrBuB4WY"
      },
      "outputs": [],
      "source": [
        "#@title TCN model training parameters\n",
        "cond_dim = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "kernel_size = 13 #@param {type:\"slider\", min:3, max:32, step:1}\n",
        "n_blocks = 5 #@param {type:\"slider\", min:2, max:30, step:1}\n",
        "dilation_growth = 8 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "n_channels = 8 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "n_iters = 2499 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "length = 228308 #@param {type:\"slider\", min:0, max:524288, step:1}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "# reshape the audio\n",
        "x_batch = x.view(1,x.shape[0],-1)\n",
        "y_batch = y.view(1,y.shape[0],-1)\n",
        "c = torch.tensor([0.0, 0.0], device=device).view(1,1,-1)\n",
        "\n",
        "# crop length\n",
        "x_batch = x_batch[:,0:1,:]\n",
        "y_batch = y_batch[:,0:1,:]\n",
        "\n",
        "_, x_ch, x_samp = x_batch.size()\n",
        "_, y_ch, y_samp = y_batch.size()\n",
        "\n",
        "# build the model\n",
        "model = TCN(\n",
        "    n_inputs=x_ch,\n",
        "    n_outputs=y_ch,\n",
        "    cond_dim=cond_dim, \n",
        "    kernel_size=kernel_size, \n",
        "    n_blocks=n_blocks, \n",
        "    dilation_growth=dilation_growth, \n",
        "    n_channels=n_channels)\n",
        "rf = model.compute_receptive_field()\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Parameters: {params*1e-3:0.3f} k\")\n",
        "print(f\"Receptive field: {rf} samples or {(rf/sample_rate)*1e3:0.1f} ms\")\n",
        "\n",
        "# setup loss function, optimizer, and scheduler\n",
        "loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
        "    fft_sizes=[32, 128, 512, 2048],\n",
        "    win_lengths=[32, 128, 512, 2048],\n",
        "    hop_sizes=[16, 64, 256, 1024])\n",
        "loss_fn_l1 = torch.nn.L1Loss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "ms1 = int(n_iters * 0.8)\n",
        "ms2 = int(n_iters * 0.95)\n",
        "milestones = [ms1, ms2]\n",
        "print(\n",
        "    \"Learning rate schedule:\",\n",
        "    f\"1:{lr:0.2e} ->\",\n",
        "    f\"{ms1}:{lr*0.1:0.2e} ->\",\n",
        "    f\"{ms2}:{lr*0.01:0.2e}\",\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones,\n",
        "    gamma=0.1,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# move tensors to GPU\n",
        "if torch.cuda.is_available():\n",
        "  model.to(device)\n",
        "  x_batch = x_batch.to(device)\n",
        "  y_batch = y_batch.to(device)\n",
        "  c = c.to(device)\n",
        "\n",
        "# pad input so that output is same size as input\n",
        "#x_pad = torch.nn.functional.pad(x_batch, (rf-1, 0))\n",
        "\n",
        "# iteratively update the weights\n",
        "pbar = tqdm(range(n_iters))\n",
        "for n in pbar:\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  start_idx = rf #np.random.randint(rf, x_batch.shape[-1]-length-1)\n",
        "  stop_idx = start_idx + length\n",
        "  x_crop = x_batch[...,start_idx-rf+1:stop_idx]\n",
        "  y_crop = y_batch[...,start_idx:stop_idx]\n",
        "\n",
        "  y_hat = model(x_crop, c)\n",
        "  loss = loss_fn(y_hat, y_crop) #+ loss_fn_l1(y_hat, y_crop)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "  if (n+1) % 1 == 0:\n",
        "    pbar.set_description(f\" Loss: {loss.item():0.3e} | \")\n",
        "\n",
        "y_hat /= y_hat.abs().max()\n",
        "\n",
        "model.eval()\n",
        "x_pad = torch.nn.functional.pad(x_batch, (rf-1, 0))\n",
        "with torch.no_grad():\n",
        "  y_hat = model(x_pad, c)\n",
        "\n",
        "input = causal_crop(x_batch.view(-1).detach().cpu().numpy(), y_hat.shape[-1])\n",
        "output = y_hat.view(-1).detach().cpu().numpy()\n",
        "target = causal_crop(y_batch.view(-1).detach().cpu().numpy(), y_hat.shape[-1])\n",
        "\n",
        "# apply highpass to output\n",
        "sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "output = scipy.signal.sosfilt(sos, output)\n",
        "\n",
        "input /= np.max(np.abs(input))\n",
        "output /= np.max(np.abs(output))\n",
        "target /= np.max(np.abs(target))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
        "librosa.display.waveshow(target, sr=sample_rate, alpha=0.5, ax=ax, label='Target')\n",
        "librosa.display.waveshow(output, sr=sample_rate, color='r', alpha=0.5, ax=ax, label='Output')\n",
        "\n",
        "print(\"Input (clean)\")\n",
        "IPython.display.display(IPython.display.Audio(data=input, rate=sample_rate))\n",
        "print(\"Target\")\n",
        "IPython.display.display(IPython.display.Audio(data=target, rate=sample_rate))\n",
        "print(\"Output\")\n",
        "IPython.display.display(IPython.display.Audio(data=output, rate=sample_rate))\n",
        "plt.legend()\n",
        "plt.show(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqxbdaQv7DD3"
      },
      "source": [
        "## 2D Plot\n",
        "Now we can generate a 2D plot of the parameter space..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "W8-AshCJYZFs"
      },
      "outputs": [],
      "source": [
        "#@title Generate plot\n",
        "size = 22 * 2\n",
        "max_cond = 5\n",
        "min_cond = -5\n",
        "values = np.zeros((size,size))\n",
        "space = np.linspace(min_cond, max_cond, num=size, dtype=np.float32)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if effect_type == \"Reverb\": \n",
        "  impulse = torch.zeros(1, 1, 65536*3)\n",
        "  impulse[..., 16384*2] = 1.2\n",
        "  impulse = impulse.to(device)\n",
        "  for xidx, x_c in enumerate(tqdm(space)):\n",
        "    for yidx, y_c in enumerate(space):\n",
        "      c = torch.tensor([x_c,y_c], device=device).view(1,1,-1).float()\n",
        "      with torch.no_grad():\n",
        "        y_hat = model(impulse, c).cpu().view(-1)\n",
        "      sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "      y_hat = scipy.signal.sosfilt(sos, y_hat.numpy())\n",
        "      rt60 = measure_rt60(y_hat)\n",
        "      rt60_sec = rt60 / sample_rate\n",
        "      values[xidx,yidx] = rt60_sec\n",
        "\n",
        "  fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
        "  img = plt.imshow(values, interpolation='nearest', origin=\"lower\")\n",
        "  ticks = np.linspace(min_cond, max_cond, num=10)\n",
        "  ticks_str = [f\"{t:0.1f}\" for t in ticks]\n",
        "  ax.set_xticks([])\n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticks([])\n",
        "  ax.set_yticklabels([])\n",
        "  cbar = fig.colorbar(img,fraction=0.046, pad=0.04)\n",
        "  cbar.set_label(r\"$T_{60}$ (sec)\")\n",
        "  ax.set_xlabel(r\"$c_0$\")\n",
        "  ax.set_ylabel(r\"$c_1$\")\n",
        "  \n",
        "elif effect_type == \"Compressor\":\n",
        "  test_signal, _ = torchaudio.load(\"piano_clean.wav\")\n",
        "  test_signal = test_signal[0,:65536*3]\n",
        "  test_signal = test_signal.view(1,1,-1)\n",
        "  test_signal = test_signal.to(device)\n",
        "\n",
        "  meter = pyln.Meter(sample_rate)\n",
        "\n",
        "  for xidx, x_c in enumerate(tqdm(space)):\n",
        "    for yidx, y_c in enumerate(space):\n",
        "      c = torch.tensor([x_c,y_c], device=device).view(1,1,-1)\n",
        "      with torch.no_grad():\n",
        "        y_hat = model(test_signal, c).cpu().view(-1)\n",
        "      sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "      y_hat = scipy.signal.sosfilt(sos, y_hat.numpy())\n",
        "      y_hat /= np.max(np.abs(y_hat))\n",
        "      dB_lufs = meter.integrated_loudness(y_hat.reshape(-1,1))\n",
        "      values[xidx,yidx] = dB_lufs\n",
        "\n",
        "  fig, ax = plt.subplots(1,1, figsize=(5,5))\n",
        "  img = plt.imshow(values, interpolation='nearest', origin=\"lower\")\n",
        "  ax.set_xticks([])\n",
        "  ax.set_xticklabels([])\n",
        "  ax.set_yticks([])\n",
        "  ax.set_yticklabels([])\n",
        "  cbar = fig.colorbar(img,fraction=0.046, pad=0.04)\n",
        "  cbar.set_label(\"dBFS LUFS\")\n",
        "  ax.set_xlabel(r\"$c_0$\")\n",
        "  ax.set_ylabel(r\"$c_1$\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OuVoSL93mLU"
      },
      "outputs": [],
      "source": [
        "fig.tight_layout()\n",
        "fig.savefig(\"plot.pdf\", dpi=300)\n",
        "files.download(\"plot.pdf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKmzWBaSJEcw"
      },
      "source": [
        "## Process new sounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuC70b_j28K5"
      },
      "outputs": [],
      "source": [
        "x_whole, sample_rate = torchaudio.load(\"acgtr_clean.wav\")\n",
        "x_whole = torch.nn.functional.pad(x_whole, (rf-1, rf-1))\n",
        "x_whole = x_whole[0,:]\n",
        "x_whole = x_whole.view(1,1,-1).to(device)\n",
        "c_rand = torch.tensor([-0.1,0.0], device=device).view(1,1,-1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  y_whole = model(0.2 * x_whole, c_rand)\n",
        "  x_whole = causal_crop(x_whole, y_whole.shape[-1])\n",
        "\n",
        "y_whole /= y_whole.abs().max()\n",
        "\n",
        "# apply high pass filter to remove DC\n",
        "sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "y_whole = scipy.signal.sosfilt(sos, y_whole.cpu().view(-1).numpy())\n",
        "\n",
        "# remove start transient\n",
        "y_whole = y_whole[4410:]\n",
        "x_whole = x_whole.view(-1)[4410:].cpu().numpy()\n",
        "\n",
        "y_whole = (y_whole * 0.8)\n",
        "IPython.display.display(IPython.display.Audio(data=x_whole, rate=sample_rate))\n",
        "IPython.display.display(IPython.display.Audio(data=y_whole, rate=sample_rate))\n",
        "\n",
        "x_whole /= np.max(np.abs(x_whole))\n",
        "y_whole /= np.max(np.abs(y_whole))\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
        "librosa.display.waveshow(y_whole, sr=sample_rate, color='r', alpha=0.5, ax=ax, label='Output')\n",
        "librosa.display.waveshow(causal_crop(x_whole, y_whole.shape[-1]), sr=sample_rate, alpha=0.5, ax=ax, label='Input')\n",
        "plt.legend()\n",
        "plt.show(fig)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb8PtWmaAL2R"
      },
      "outputs": [],
      "source": [
        "#torch.save(model, \"./reverb_full.pt\")\n",
        "#torch.save(model, \"./compressor_full.pt\")\n",
        "torch.save(model, \"./delay_full.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dcEqkhjs54Mw"
      },
      "outputs": [],
      "source": [
        "#files.download(\"./reverb_full.pt\")\n",
        "#files.download(\"./compressor_full.pt\")\n",
        "files.download(\"./delay_full.pt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXUv5gPaEw4U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "A6x_gvWLIC8c",
        "SqxbdaQv7DD3",
        "UKmzWBaSJEcw"
      ],
      "name": "Steerable discovery of neural audio effects.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
