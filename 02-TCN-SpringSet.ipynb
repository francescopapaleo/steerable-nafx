{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BtCdDb87-5jY"
      },
      "source": [
        "# Evaluation of the dataset with the baseline model\n",
        "\n",
        "Sources from https://github.com/csteinmetz1/steerable-nafx/blob/main/steerable-nafx.ipynb\n",
        "\n",
        "For this experiment we will use the concatenated subset to train the model and get some results to compare with a pretrained model.\n",
        "We will use a TCN model with FiLM layer. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WD2zLjdiILd-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vqu3t1D8_ePE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import librosa.display\n",
        "import auraloss\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "from pathlib import Path\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-09-09 18:30:04--  https://zenodo.org/record/3746119/files/plate-spring.zip\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.124.72\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.124.72|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 504370887 (481M) [application/octet-stream]\n",
            "Saving to: ‘../data/raw/plate-spring.zip’\n",
            "\n",
            "plate-spring.zip    100%[===================>] 481,00M  1,74MB/s    in 5m 29s  \n",
            "\n",
            "2023-09-09 18:35:33 (1,46 MB/s) - ‘../data/raw/plate-spring.zip’ saved [504370887/504370887]\n",
            "\n",
            "Archive:  ../datasets/plate-spring.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of ../datasets/plate-spring.zip or\n",
            "        ../datasets/plate-spring.zip.zip, and cannot find ../datasets/plate-spring.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "# Download dataset\n",
        "if os.path.exists('../datasets/plate-spring') == False:\n",
        "    !wget -P ../data/raw/ https://zenodo.org/record/3746119/files/plate-spring.zip\n",
        "    !unzip ../datasets/plate-spring.zip -d ../datasets/\n",
        "else:\n",
        "    print('Dataset already downloaded.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global variables\n",
        "DATA_DIR = '../datasets/plate-spring/spring'\n",
        "CONVERTED_DIR = Path(\"audio/springset_converted\")\n",
        "PROCESSED_DIR = Path(\"audio/processed\")\n",
        "MODELS_DIR = \"models/\"\n",
        "PLOTS_DIR = \"plots/\"\n",
        "\n",
        "\n",
        "sample_rate = 16000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e7xVSq7gJvgU"
      },
      "source": [
        "## Implementation of the TCN model with FiLM layers for training and inference "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aUepU06_zHk"
      },
      "outputs": [],
      "source": [
        "def causal_crop(x, length: int):\n",
        "    if x.shape[-1] != length:\n",
        "        stop = x.shape[-1] - 1\n",
        "        start = stop - length\n",
        "        x = x[..., start:stop]\n",
        "    return x\n",
        "\n",
        "class FiLM(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        cond_dim,  # dim of conditioning input\n",
        "        num_features,  # dim of the conv channel\n",
        "        batch_norm=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.batch_norm = batch_norm\n",
        "        if batch_norm:\n",
        "            self.bn = torch.nn.BatchNorm1d(num_features, affine=False)\n",
        "        self.adaptor = torch.nn.Linear(cond_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "\n",
        "        cond = self.adaptor(cond)\n",
        "        g, b = torch.chunk(cond, 2, dim=-1)\n",
        "        g = g.permute(0, 2, 1)\n",
        "        b = b.permute(0, 2, 1)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            x = self.bn(x)  # apply BatchNorm without affine\n",
        "        x = (x * g) + b  # then apply conditional affine\n",
        "\n",
        "        return x\n",
        "\n",
        "class TCNBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, dilation, cond_dim=0, activation=True):\n",
        "    super().__init__()\n",
        "    self.conv = torch.nn.Conv1d(\n",
        "        in_channels, \n",
        "        out_channels, \n",
        "        kernel_size, \n",
        "        dilation=dilation, \n",
        "        padding=0, #((kernel_size-1)//2)*dilation,\n",
        "        bias=True)\n",
        "    if cond_dim > 0:\n",
        "      self.film = FiLM(cond_dim, out_channels, batch_norm=False)\n",
        "    if activation:\n",
        "      #self.act = torch.nn.Tanh()\n",
        "      self.act = torch.nn.PReLU()\n",
        "    self.res = torch.nn.Conv1d(in_channels, out_channels, 1, bias=False)\n",
        "\n",
        "  def forward(self, x, c=None):\n",
        "    x_in = x\n",
        "    x = self.conv(x)\n",
        "    if hasattr(self, \"film\"):\n",
        "      x = self.film(x, c)\n",
        "    if hasattr(self, \"act\"):\n",
        "      x = self.act(x)\n",
        "    x_res = causal_crop(self.res(x_in), x.shape[-1])\n",
        "    x = x + x_res\n",
        "\n",
        "    return x\n",
        "\n",
        "class TCN(torch.nn.Module):\n",
        "  def __init__(self, n_inputs=1, n_outputs=1, n_blocks=10, kernel_size=13, n_channels=64, dilation_growth=4, cond_dim=0):\n",
        "    super().__init__()\n",
        "    self.kernel_size = kernel_size\n",
        "    self.n_channels = n_channels\n",
        "    self.dilation_growth = dilation_growth\n",
        "    self.n_blocks = n_blocks\n",
        "    self.stack_size = n_blocks\n",
        "\n",
        "    self.blocks = torch.nn.ModuleList()\n",
        "    for n in range(n_blocks):\n",
        "      if n == 0:\n",
        "        in_ch = n_inputs\n",
        "        out_ch = n_channels\n",
        "        act = True\n",
        "      elif (n+1) == n_blocks:\n",
        "        in_ch = n_channels\n",
        "        out_ch = n_outputs\n",
        "        act = True\n",
        "      else:\n",
        "        in_ch = n_channels\n",
        "        out_ch = n_channels\n",
        "        act = True\n",
        "      \n",
        "      dilation = dilation_growth ** n\n",
        "      self.blocks.append(TCNBlock(in_ch, out_ch, kernel_size, dilation, cond_dim=cond_dim, activation=act))\n",
        "\n",
        "  def forward(self, x, c=None):\n",
        "    for block in self.blocks:\n",
        "      x = block(x, c)\n",
        "\n",
        "    return x\n",
        "  \n",
        "  def compute_receptive_field(self):\n",
        "    \"\"\"Compute the receptive field in samples.\"\"\"\n",
        "    rf = self.kernel_size\n",
        "    for n in range(1, self.n_blocks):\n",
        "        dilation = self.dilation_growth ** (n % self.stack_size)\n",
        "        rf = rf + ((self.kernel_size - 1) * dilation)\n",
        "    return rf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wb-wyV-5_-mi"
      },
      "source": [
        "## Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azxy12q2--Te",
        "outputId": "0cd8bd28-4d6a-4fe2-914e-33d9343da01f"
      },
      "outputs": [],
      "source": [
        "#@title Load the pre-trained model\n",
        "model_path = os.path.join(MODELS_DIR, \"reverb_full.pt\")\n",
        "model_verb = torch.load(model_path, map_location=\"cpu\").eval()\n",
        "print(model_verb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "QWSEHdbt_lNJ",
        "outputId": "7d010591-d035-4a03-b05c-ff470004fb75"
      },
      "outputs": [],
      "source": [
        "#@title Load validation data for testing\n",
        "process_file = os.path.join(CONVERTED_DIR, 'stack_X_test.wav')\n",
        "x_p, sample_rate = torchaudio.load(process_file)\n",
        "print(process_file, x_p.shape)\n",
        "ipd.Audio(data=x_p, rate=sample_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lIlsefCnKR57"
      },
      "source": [
        "Now set the audio effect parameters. Here are some more insights into the controls:\n",
        "\n",
        "*   `effect_type` - Choose from one of the pre-trained models.\n",
        "*   `gain_dB` - Adjust the input gain. This can have a big effect since the effects are very nonlinear.\n",
        "*   `c0` and `c1` - These are the effect controls which will adjust perceptual aspects of the effect, depending on the effect type. Very large values will often result in more extreme effects.\n",
        "*   `mix` - Control the wet/dry mix of the effect.\n",
        "*   `width` - Increase stereo width of the effect.\n",
        "*   `max_length` - If you uploaded a very long file this will truncate it.\n",
        "*   `stereo` - Convert mono input to stereo output.\n",
        "*   `tail` - If checked, we will also compute the effect tail (nice for reverbs).\n",
        "*   `output_file` - Output file path. It avoids overwriting the processed files. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "TfJ00sNNAgOr",
        "outputId": "fc957fdd-c0ba-40df-9e84-309558fe5172"
      },
      "outputs": [],
      "source": [
        "#@title Setting parameters and processing\n",
        "gain_dB = 0 #@param {type:\"slider\", min:-24, max:24, step:0.1}\n",
        "c0 = -10 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "c1 = -10 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "mix = 100 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "width = 50 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "max_length = 129 #@param {type:\"slider\", min:5, max:130, step:1}\n",
        "stereo = False #@param {type:\"boolean\"}\n",
        "tail = True #@param {type:\"boolean\"}\n",
        "save_pred_file = os.path.join(PROCESSED_DIR, \"pretrained_y_hat.wav\") #@param {type: \"string\"}\n",
        "\n",
        "# measure the receptive field\n",
        "pt_model_rf = model_verb.compute_receptive_field()\n",
        "\n",
        "# crop input signal if needed\n",
        "max_samples = int(sample_rate * max_length)\n",
        "x_p_crop = x_p[:,:max_samples]\n",
        "chs = x_p_crop.shape[0]\n",
        "\n",
        "# if mono and stereo requested\n",
        "if chs == 1 and stereo:\n",
        "  x_p_crop = x_p_crop.repeat(2,1)\n",
        "  chs = 2\n",
        "\n",
        "# pad the input signal\n",
        "front_pad = pt_model_rf-1\n",
        "back_pad = 0 if not tail else front_pad\n",
        "x_p_pad = torch.nn.functional.pad(x_p_crop, (front_pad, back_pad))\n",
        "\n",
        "# design highpass filter\n",
        "sos = scipy.signal.butter(\n",
        "    8, \n",
        "    20.0, \n",
        "    fs=sample_rate, \n",
        "    output=\"sos\", \n",
        "    btype=\"highpass\"\n",
        ")\n",
        "\n",
        "# compute linear gain \n",
        "gain_ln = 10 ** (gain_dB / 20.0)\n",
        "\n",
        "# process audio with pre-trained model\n",
        "with torch.no_grad():\n",
        "  y_hat = torch.zeros(x_p_crop.shape[0], x_p_crop.shape[1] + back_pad)\n",
        "  for n in range(chs):\n",
        "    if n == 0:\n",
        "      factor = (width*5e-3)\n",
        "    elif n == 1:\n",
        "      factor = -(width*5e-3)\n",
        "    c = torch.tensor([float(c0+factor), float(c1+factor)]).view(1,1,-1)\n",
        "    y_hat_ch = model_verb(gain_ln * x_p_pad[n,:].view(1,1,-1), c)\n",
        "    y_hat_ch = scipy.signal.sosfilt(sos, y_hat_ch.view(-1).numpy())\n",
        "    y_hat_ch = torch.tensor(y_hat_ch)\n",
        "    y_hat[n,:] = y_hat_ch\n",
        "\n",
        "# pad the dry signal \n",
        "x_dry = torch.nn.functional.pad(x_p_crop, (0,back_pad))\n",
        "\n",
        "# normalize each first\n",
        "y_hat /= y_hat.abs().max()\n",
        "x_dry /= x_dry.abs().max()\n",
        "\n",
        "# mix\n",
        "mix = mix/100.0\n",
        "y_hat = (mix * y_hat) + ((1-mix) * x_dry)\n",
        "\n",
        "# remove transient\n",
        "y_hat = y_hat[...,8192:]\n",
        "y_hat /= y_hat.abs().max()\n",
        "\n",
        "# save and preview the audio\n",
        "torchaudio.save(save_pred_file, y_hat.view(chs,-1), sample_rate, encoding=\"PCM_S\", bits_per_sample=16)\n",
        "ipd.Audio(save_pred_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def overlap_waveforms(output, target, sample_rate, start, end, title):    \n",
        "    o_zoom = output[start:end]\n",
        "    t_zoom = target[start:end]\n",
        "\n",
        "    # create time vector\n",
        "    time = range(start, end)\n",
        "\n",
        "    plt.figure(figsize=(12,4))\n",
        "    plt.plot(time, o_zoom, alpha=0.5, label=\"Output\")\n",
        "    plt.plot(time, t_zoom, alpha=0.5, label=\"Target\")\n",
        "    plt.xlabel(\"Time (samples)\")\n",
        "    plt.ylabel(\"Amplitude\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start = 11000\n",
        "end = 12000\n",
        "\n",
        "N = 512\n",
        "hop_size = 256\n",
        "\n",
        "o_p, sample_rate = torchaudio.load(PROCESSED_DIR / \"pretrained_y_hat.wav\")\n",
        "t_p, t_sr = torchaudio.load(CONVERTED_DIR / \"stack_Y_test.wav\")\n",
        "\n",
        "output = o_p.numpy().squeeze()\n",
        "target = t_p.numpy().squeeze()\n",
        "\n",
        "overlap_waveforms(output, target, sample_rate, start, end, \"Waveform comparison\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can notice a time alignment issue between the target values and the model's outputs. This could potentially be attributed to various factors, one of them being that the data used for evaluation might not be the same as the one used during the training process of the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmc3Z1riLuJN"
      },
      "source": [
        "## Train a new model with the chosen dataset\n",
        "\n",
        "Using the same model architecture, we will train a new model with the chosen dataset. \n",
        "Same hyperparameters will be used for the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JvwWOCQPchB",
        "outputId": "3919c95a-31fd-438a-cc5e-8738c5ec8ac0"
      },
      "outputs": [],
      "source": [
        "# Upload clean sound (Xtrain.wav)\n",
        "input_file = os.path.join(CONVERTED_DIR, 'stack_X_train.wav')\n",
        "x, sample_rate = torchaudio.load(input_file)\n",
        "print(input_file, x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcy84CxxX2Vp",
        "outputId": "19a1bc1e-fc9d-482c-a893-b35b9a06cfb2"
      },
      "outputs": [],
      "source": [
        "#@title Upload processed sound (Ytrain_0.wav) that will be the target\n",
        "save_pred_file = os.path.join(CONVERTED_DIR, 'stack_Y_train.wav')\n",
        "y, sample_rate = torchaudio.load(save_pred_file)\n",
        "\n",
        "if not y.shape[-1] == x.shape[-1]:\n",
        "  print(f\"Input and output files are different lengths! Found clean: {x.shape[-1]} processed: {y.shape[-1]}.\")\n",
        "  if y.shape[-1] > x.shape[-1]:\n",
        "    print(f\"Cropping target...\")\n",
        "    y = y[:,:x.shape[-1]]\n",
        "  else:\n",
        "    print(f\"Cropping input...\")\n",
        "    x = x[:,:y.shape[-1]]\n",
        "\n",
        "print(save_pred_file, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6oYf_KhtBqB"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7fb358e96b6047e7a7269bd2e4dbe698",
            "3b08c691f6a54ac6be07bf74dcbbd6e3",
            "9afd3c44061a46419320a9c061a2f7f3",
            "8942cf0b80f14de1b4ff8abb949e7e06",
            "0055c99346bb4ca48515739e3df95328",
            "d718a252d5714a9a8a83a368c79b7e9e",
            "d48e31230164475fbb0e8b824a8fb873",
            "bab92db3dc8a43e3aa1f5f7c5d29b221",
            "666de91617bd4128be4e5311d1ffd6e5",
            "cf78bf4ebb1e482d8ae7e40ba8bc2d9c",
            "9c4b53b0744c4f4d92dfa37fe226d39f"
          ]
        },
        "id": "ZffPT0UhYnGd",
        "outputId": "e2037733-03ad-4a83-bf98-e75a6aa50402"
      },
      "outputs": [],
      "source": [
        "#TCN model hyperparameters and launch training\n",
        "cond_dim = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "kernel_size = 9 #@param {type:\"slider\", min:3, max:32, step:1}\n",
        "n_blocks = 5 #@param {type:\"slider\", min:2, max:30, step:1}\n",
        "dilation_growth = 10 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "n_channels = 32 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "n_iters = 2500 #@param {type:\"slider\", min:0, max:10000, step:1}\n",
        "length = 262144 #@param {type:\"slider\", min:0, max:524288, step:1}\n",
        "lr = 0.001 #@param {type:\"number\"}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "PYTORCH_NO_CUDA_MEMORY_CACHING=1\n",
        "\n",
        "# reshape the audio\n",
        "x_batch = x.view(1,x.shape[0],-1)\n",
        "y_batch = y.view(1,y.shape[0],-1)\n",
        "c = torch.tensor([0.0, 0.0], device=device).view(1,1,-1)\n",
        "\n",
        "# crop length\n",
        "x_batch = x_batch[:,0:1,:]\n",
        "y_batch = y_batch[:,0:1,:]\n",
        "\n",
        "_, x_ch, x_samp = x_batch.size()\n",
        "_, y_ch, y_samp = y_batch.size()\n",
        "\n",
        "# build the model\n",
        "model = TCN(\n",
        "    n_inputs=x_ch,\n",
        "    n_outputs=y_ch,\n",
        "    cond_dim=cond_dim, \n",
        "    kernel_size=kernel_size, \n",
        "    n_blocks=n_blocks, \n",
        "    dilation_growth=dilation_growth, \n",
        "    n_channels=n_channels)\n",
        "rf = model.compute_receptive_field()\n",
        "params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Parameters: {params*1e-3:0.3f} k\")\n",
        "print(f\"Receptive field: {rf} samples or {(rf/sample_rate)*1e3:0.1f} ms\")\n",
        "print(model)\n",
        "\n",
        "# setup loss function, optimizer, and scheduler\n",
        "loss_fn = auraloss.freq.MultiResolutionSTFTLoss(\n",
        "    fft_sizes=[32, 128, 512, 2048],\n",
        "    win_lengths=[32, 128, 512, 2048],\n",
        "    hop_sizes=[16, 64, 256, 1024])\n",
        "loss_fn_l1 = torch.nn.L1Loss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
        "ms1 = int(n_iters * 0.8)\n",
        "ms2 = int(n_iters * 0.95)\n",
        "milestones = [ms1, ms2]\n",
        "print(\n",
        "    \"Learning rate schedule:\",\n",
        "    f\"1:{lr:0.2e} ->\",\n",
        "    f\"{ms1}:{lr*0.1:0.2e} ->\",\n",
        "    f\"{ms2}:{lr*0.01:0.2e}\",\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer,\n",
        "    milestones,\n",
        "    gamma=0.1,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "# move tensors to GPU\n",
        "if torch.cuda.is_available():\n",
        "  model.to(device)\n",
        "  x_batch = x_batch.to(device)\n",
        "  y_batch = y_batch.to(device)\n",
        "  c = c.to(device)\n",
        "\n",
        "# pad input so that output is same size as input\n",
        "x_pad = torch.nn.functional.pad(x_batch, (rf-1, 0))\n",
        "\n",
        "# iteratively update the weights\n",
        "pbar = tqdm(range(n_iters))\n",
        "for n in pbar:\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  start_idx = rf #np.random.randint(rf, x_batch.shape[-1]-length-1)\n",
        "  stop_idx = start_idx + length\n",
        "  x_crop = x_batch[...,start_idx-rf+1:stop_idx]\n",
        "  y_crop = y_batch[...,start_idx:stop_idx]\n",
        "\n",
        "  y_hat = model(x_crop, c)\n",
        "  loss = loss_fn(y_hat, y_crop) #+ loss_fn_l1(y_hat, y_crop)\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "  scheduler.step()\n",
        "  if (n+1) % 1 == 0:\n",
        "    pbar.set_description(f\" Loss: {loss.item():0.3e} | \")\n",
        "\n",
        "y_hat /= y_hat.abs().max()\n",
        "\n",
        "model.eval()\n",
        "x_pad = torch.nn.functional.pad(x_batch, (rf-1, 0))\n",
        "with torch.no_grad():\n",
        "  y_hat = model(x_pad, c)\n",
        "\n",
        "input = causal_crop(x_batch.view(-1).detach().cpu().numpy(), y_hat.shape[-1])\n",
        "output = y_hat.view(-1).detach().cpu().numpy()\n",
        "target = causal_crop(y_batch.view(-1).detach().cpu().numpy(), y_hat.shape[-1])\n",
        "\n",
        "# apply highpass to output\n",
        "sos = scipy.signal.butter(8, 20.0, fs=sample_rate, output=\"sos\", btype=\"highpass\")\n",
        "output = scipy.signal.sosfilt(sos, output)\n",
        "\n",
        "input /= np.max(np.abs(input))\n",
        "output /= np.max(np.abs(output))\n",
        "target /= np.max(np.abs(target))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time = 3.9 #@param {type:\"slider\", min:0, max:30, step:0.1}\n",
        "center = 54 #@param {type:\"slider\", min:0, max:120, step:1}\n",
        "time_window = [center - time, center + time]\n",
        "\n",
        "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, figsize=(10, 4))\n",
        "\n",
        "librosa.display.waveshow(target,\n",
        "                         sr=sample_rate, \n",
        "                         alpha=0.7, \n",
        "                         ax=ax, \n",
        "                         label='Target')\n",
        "librosa.display.waveshow(output, \n",
        "                         sr=sample_rate,  \n",
        "                         alpha=0.7, \n",
        "                         ax=ax, \n",
        "                         label='Output')\n",
        "ax.set_title(\"New Model\")\n",
        "\n",
        "ax.set_xlabel('Time (min)')\n",
        "ax.set_ylabel('Amplitude')\n",
        "ax.set_xlim(time_window)\n",
        "ax.grid(True)\n",
        "\n",
        "ax.legend(loc='upper right')\n",
        "plt.tight_layout()    \n",
        "plt.savefig(os.path.join(PLOTS_DIR,'new_model_out_wave.png'))\n",
        "print(\"Done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOU2zqaG0ocu"
      },
      "outputs": [],
      "source": [
        "#Save and load your spring model to inference with CPU\n",
        "model_fn = os.path.join(MODELS_DIR, \"spring_reverb_new_nb_02.pt\")\n",
        "torch.save(model, model_fn)\n",
        "my_spring_reverb_model = torch.load(model_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAqA2JNYY1fm",
        "outputId": "9ebd8609-0e9f-4db4-fd56-1d5262255e3d"
      },
      "outputs": [],
      "source": [
        "save_pred_file = os.path.join(CONVERTED_DIR, 'stacked_Xvalidation.wav')\n",
        "x_p, sample_rate = torchaudio.load(process_file)\n",
        "print(process_file, x_p.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of the new model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "NWZcI19hv8yL",
        "outputId": "be10cac5-bc1e-44d9-8a57-31ad8b49bab6"
      },
      "outputs": [],
      "source": [
        "# Make an Inference\n",
        "\n",
        "gain_dB = 0 #@param {type:\"slider\", min:-24, max:24, step:0.1}\n",
        "c0 = -10 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "c1 = -10 #@param {type:\"slider\", min:-10, max:10, step:0.1}\n",
        "mix = 100 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "width = 50 #@param {type:\"slider\", min:0, max:100, step:1}\n",
        "max_length = 128 #@param {type:\"slider\", min:5, max:128, step:1}\n",
        "stereo = False #@param {type:\"boolean\"}\n",
        "tail = True #@param {type:\"boolean\"}\n",
        "output_file = os.path.join(PROCESSED_DIR, \"new_model_out_inference.wav\") #@param {type: \"string\"}\n",
        "\n",
        "model.eval()\n",
        "# measure the receptive field\n",
        "model_rf = model.compute_receptive_field()\n",
        "\n",
        "# crop input signal if needed\n",
        "max_samples = int(sample_rate * max_length)\n",
        "x_p_crop = x_p[:,:max_samples]\n",
        "chs = x_p_crop.shape[0]\n",
        "\n",
        "# if mono and stereo requested\n",
        "if chs == 1 and stereo:\n",
        "  x_p_crop = x_p_crop.repeat(2, 1)\n",
        "  chs = 2\n",
        "\n",
        "# pad the input signal\n",
        "front_pad = model_rf - 1\n",
        "back_pad = 0 if not tail else front_pad\n",
        "x_p_pad = torch.nn.functional.pad(x_p_crop, (front_pad, back_pad))\n",
        "\n",
        "# design highpass filter\n",
        "sos = scipy.signal.butter(\n",
        "    8, \n",
        "    20.0, \n",
        "    fs=sample_rate, \n",
        "    output=\"sos\", \n",
        "    btype=\"highpass\"\n",
        ")\n",
        "\n",
        "# compute linear gain \n",
        "gain_ln = 10 ** (gain_dB / 20.0)\n",
        "\n",
        "# move data to cuda\n",
        "x_p_crop = x_p_crop.to(device)\n",
        "x_p_pad = x_p_pad.to(device)\n",
        "\n",
        "# process audio with pre-trained model\n",
        "with torch.no_grad():\n",
        "  y_hat = torch.zeros(x_p_crop.shape[0], x_p_crop.shape[1] + back_pad, device=device)\n",
        "  for n in range(chs):\n",
        "    if n == 0:\n",
        "      factor = (width * 5e-3)\n",
        "    elif n == 1:\n",
        "      factor = -(width * 5e-3)\n",
        "    c = torch.tensor([float(c0+factor), float(c1+factor)], device=device).view(1, 1, -1).float()\n",
        "    y_hat_ch = model(gain_ln * x_p_pad[n, :].view(1, 1, -1), c).cpu()\n",
        "    y_hat_ch = scipy.signal.sosfilt(sos, y_hat_ch.view(-1).numpy())\n",
        "    y_hat_ch = torch.tensor(y_hat_ch)\n",
        "    y_hat[n,:] = y_hat_ch\n",
        "\n",
        "# pad the dry signal \n",
        "x_dry = torch.nn.functional.pad(x_p_crop, (0, back_pad))\n",
        "\n",
        "# normalize each first\n",
        "y_hat /= y_hat.abs().max()\n",
        "x_dry /= x_dry.abs().max()\n",
        "\n",
        "# mix\n",
        "mix = mix / 100.0\n",
        "y_hat = (mix * y_hat) + ((1 - mix) * x_dry)\n",
        "\n",
        "# remove transient\n",
        "y_hat = y_hat[..., 8192:]\n",
        "y_hat /= y_hat.abs().max()\n",
        "\n",
        "torchaudio.save(output_file, y_hat.view(chs, -1).to(\"cpu\"), sample_rate, encoding=\"PCM_S\", bits_per_sample=16)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define some metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1gZgo009hsi"
      },
      "outputs": [],
      "source": [
        "#@title Define ESR and MSE metrics\n",
        "def error_to_signal(y, y_pred):\n",
        "    \"\"\"\n",
        "    Error to signal ratio with pre-emphasis filter:\n",
        "    https://www.mdpi.com/2076-3417/10/3/766/htm\n",
        "    \"\"\"\n",
        "    # y, y_pred = pre_emphasis_filter(y), pre_emphasis_filter(y_pred)\n",
        "    \n",
        "    # Pad inputs to same size\n",
        "    max_len = max(y.shape[1], y_pred.shape[1])\n",
        "    y = torch.nn.functional.pad(y, (0, max_len - y.shape[1]))\n",
        "    y_pred = torch.nn.functional.pad(y_pred, (0, max_len - y_pred.shape[1]))\n",
        "    \n",
        "    # Compute error to signal ratio\n",
        "    error = torch.sum(torch.pow(y - y_pred, 2))\n",
        "    signal = torch.sum(torch.pow(y, 2))\n",
        "    esr = error / (signal + 1e-10)\n",
        "    return esr\n",
        "\n",
        "\n",
        "def pre_emphasis_filter(x, coeff=0.95):\n",
        "    return torch.cat((x[:, 0:1], x[:, 1:] - coeff * x[:, :-1]), dim=1)\n",
        "\n",
        "\n",
        "def mean_square_error(y, y_pred):\n",
        "    \"\"\"\n",
        "    Compute mean square error between y and y_pred\n",
        "    \"\"\"\n",
        "    # Pad inputs to same size\n",
        "    max_len = max(y.shape[1], y_pred.shape[1])\n",
        "    y = torch.nn.functional.pad(y, (0, max_len - y.shape[1]))\n",
        "    y_pred = torch.nn.functional.pad(y_pred, (0, max_len - y_pred.shape[1]))\n",
        "    \n",
        "    # Compute mean square error\n",
        "    mse = torch.nn.functional.mse_loss(y_pred, y)\n",
        "    \n",
        "    return mse.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xjl2FGT2gpTO",
        "outputId": "a809aa02-16ac-4472-b6ce-f420b09321cd"
      },
      "outputs": [],
      "source": [
        "#@title Compute ESR and MSE \n",
        "ground_truth = os.path.join(CONVERTED_DIR, 'stacked_Yvalidation_0.wav')\n",
        "pred_1 = os.path.join(PROCESSED_DIR, 'pretrained-out.wav')\n",
        "pred_2 = os.path.join(PROCESSED_DIR, 'new_model_out.wav')\n",
        "\n",
        "print(torchaudio.info(ground_truth))\n",
        "print(torchaudio.info(pred_1))\n",
        "print(torchaudio.info(pred_2))\n",
        "\n",
        "gt, sr = torchaudio.load(ground_truth)\n",
        "p1, sr = torchaudio.load(pred_1)\n",
        "p2, sr = torchaudio.load(pred_2)\n",
        "\n",
        "max_len = max(gt.shape[-1], p1.shape[-1], p2.shape[-1])\n",
        "\n",
        "# Pad the waveforms with zeros to the length of the longest waveform\n",
        "gt = torch.nn.functional.pad(gt, (0, max_len-gt.shape[-1]), 'constant', 0)\n",
        "p1 = torch.nn.functional.pad(p1, (0, max_len-p1.shape[-1]), 'constant', 0)\n",
        "p2 = torch.nn.functional.pad(p2, (0, max_len-p2.shape[-1]), 'constant', 0)\n",
        "\n",
        "p1_esr = error_to_signal(gt, p1)\n",
        "p2_esr = error_to_signal(gt, p2)\n",
        "\n",
        "print(f\"First model ESR: {p1_esr}\")\n",
        "print(f\"Second model ESR: {p2_esr}\")\n",
        "\n",
        "mse_1 = mean_square_error(gt, p1)\n",
        "mse_2 = mean_square_error(gt, p2)\n",
        "\n",
        "print(f\"First model MSE: {mse_1}\")\n",
        "print(f\"Second model MSE: {mse_2}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "Zt9_88x-lJtL",
        "outputId": "1f634fd6-3814-4cb1-e252-5c1991f9ff65"
      },
      "outputs": [],
      "source": [
        "#@title Plot ESR and MSE\n",
        "labels = ['Pretrained', 'New Model']\n",
        "esr = [p1_esr, p2_esr]\n",
        "mse = [mse_1, mse_2]\n",
        "colors = ['red', 'green',]\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10, 3))\n",
        "\n",
        "# Plot bar charts\n",
        "ax1.bar(labels, esr, color=colors, alpha=0.6, width=0.4)\n",
        "ax2.bar(labels, mse, color=colors, alpha=0.6, width=0.4)\n",
        "\n",
        "# Set titles and labels\n",
        "ax1.set_title('Error-to-Signal Ratio')\n",
        "ax1.set_ylabel('ESR')\n",
        "ax1.grid(True)\n",
        "\n",
        "ax2.set_title('Mean Square Error')\n",
        "ax2.set_ylabel('MSE')\n",
        "ax2.grid(True)\n",
        "\n",
        "# Add annotations\n",
        "for i, value in enumerate(esr):\n",
        "    ax1.annotate('{:.4f}'.format(value), xy=(i, value), ha='center', va='top')\n",
        "\n",
        "for i, value in enumerate(mse):\n",
        "    ax2.annotate('{:.4f}'.format(value), xy=(i, value), ha='center', va='top')\n",
        "\n",
        "# Display the plot\n",
        "fig.subplots_adjust(wspace=0.4)\n",
        "plt.savefig('plots/bars.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 906
        },
        "id": "u7vvmMUtD-p4",
        "outputId": "399e653d-b690-45b7-c5d1-df1e0b0235b5"
      },
      "outputs": [],
      "source": [
        "#@title Plot the waveforms\n",
        "\n",
        "fig, axs = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(10,10))\n",
        "\n",
        "axs[0].set_title('Comparison of the Y validation with the first prediction')\n",
        "librosa.display.waveshow(gt.numpy(), sr=sample_rate, ax=axs[0], color='b', alpha=0.8, label='Y validation')\n",
        "librosa.display.waveshow(p1.numpy(), sr=sample_rate, ax=axs[0], color='r', alpha=0.5, label='Pretrained model')\n",
        "\n",
        "axs[1].set_title('Comparison of the Y validation with the second prediction')\n",
        "librosa.display.waveshow(gt.numpy(), sr=sample_rate, ax=axs[1], color='b', alpha=0.8, label='Y validation')\n",
        "librosa.display.waveshow(p2.numpy(), sr=sample_rate, ax=axs[1], color='g', alpha=0.5, label='New model')\n",
        "\n",
        "axs[2].set_title('Comparison of the two predictions')\n",
        "librosa.display.waveshow(p1.numpy(), sr=sample_rate, ax=axs[2], color='r', alpha=0.5, label='Pretrained model')\n",
        "librosa.display.waveshow(p2.numpy(), sr=sample_rate, ax=axs[2], color='g', alpha=0.5, label='New model')\n",
        "\n",
        "# set the x and y labels, grid and legend for all subplots\n",
        "for i, ax in enumerate(axs):\n",
        "    ax.set_xlabel('Time (min)')\n",
        "    ax.set_ylabel('Amplitude')\n",
        "    ax.grid(True)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # Save just the portion _inside_ the second axis's boundaries\n",
        "    # extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "    # fig.savefig(f'./imgs/ax{i}-wave.png', bbox_inches=extent.expanded(1.3, 1.3))\n",
        "\n",
        "# show the figure\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "fig.savefig('plots/all-waveforms.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "THDEnTy0SUWn",
        "outputId": "85ec85d1-1a76-4cd0-a47e-68608d1c7df4"
      },
      "outputs": [],
      "source": [
        "#@title Zoom waveform's time frames\n",
        "time = 3.9 #@param {type:\"slider\", min:0, max:30, step:0.1}\n",
        "center = 54 #@param {type:\"slider\", min:0, max:120, step:1}\n",
        "\n",
        "time_window = [center - time, center + time]\n",
        "\n",
        "fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True, \n",
        "                        figsize=(10, 4))\n",
        "\n",
        "# Plot first subplot\n",
        "librosa.display.waveshow(gt.numpy(), sr=sample_rate, \n",
        "                         alpha=0.8, ax=axs[0], color='b', \n",
        "                         label='Y validation')\n",
        "librosa.display.waveshow(p1.numpy(), sr=sample_rate,  \n",
        "                         alpha=0.5, ax=axs[0], color='r', \n",
        "                         label='Pretrained Model')\n",
        "axs[0].set_title(\"Comparison of the Y validation with the first prediction\")\n",
        "\n",
        "# Plot second subplot\n",
        "librosa.display.waveshow(gt.numpy(), sr=sample_rate, \n",
        "                         alpha=0.8, ax=axs[1], color='b', \n",
        "                         label='Y validation')\n",
        "librosa.display.waveshow(p2.numpy(), sr=sample_rate,  \n",
        "                         alpha=0.5, ax=axs[1], color='g', \n",
        "                         label='New Model')\n",
        "axs[1].set_title(\"Comparison of the Y validation with the second prediction\")\n",
        "\n",
        "# set the x and y labels, grid and legend for all subplots\n",
        "for i, ax in enumerate(axs):\n",
        "    ax.set_xlabel('Time (min)')\n",
        "    ax.set_ylabel('Amplitude')\n",
        "    ax.grid(True)\n",
        "    ax.legend(loc='upper right')\n",
        "    ax.set_xlim(time_window)\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/waveforms-time-frame.png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "Py9BA2IU-spG",
        "outputId": "fa8bbf52-31fa-45ad-e23d-9493f00551da"
      },
      "outputs": [],
      "source": [
        "#@title Zoom spectrograms' time frames\n",
        "time = 1.9 #@param {type:\"slider\", min:0, max:30, step:0.1}\n",
        "center = 54 #@param {type:\"slider\", min:0, max:120, step:1}\n",
        "\n",
        "time_window = [center - time, center + time]\n",
        "\n",
        "fig, axs = plt.subplots(\n",
        "    nrows=1, ncols=3, sharey=True,\n",
        "    figsize=(10, 4))\n",
        "\n",
        "dgt = librosa.amplitude_to_db(\n",
        "    np.abs(librosa.stft(gt.numpy().squeeze())), ref=np.max)\n",
        "dp1 = librosa.amplitude_to_db(\n",
        "    np.abs(librosa.stft(p1.numpy().squeeze())), ref=np.max)\n",
        "dp2 = librosa.amplitude_to_db(\n",
        "    np.abs(librosa.stft(p2.numpy().squeeze())), ref=np.max)\n",
        "\n",
        "librosa.display.specshow(\n",
        "    dgt, sr=sample_rate, x_axis='time', ax=axs[0], \n",
        "    cmap='hot', label='Y validation')\n",
        "axs[0].set_title('Y validation')\n",
        "\n",
        "librosa.display.specshow(\n",
        "    dp1, sr=sample_rate, x_axis='time', ax=axs[1], \n",
        "    cmap='hot', label='Pretrained Model')\n",
        "axs[1].set_title('Pretrained Model')\n",
        "\n",
        "librosa.display.specshow(\n",
        "    dp2, sr=sample_rate, x_axis='time', ax=axs[2], \n",
        "    cmap='hot', label='New Model')\n",
        "axs[2].set_title('New Model')\n",
        "\n",
        "# set the x and y labels, grid and legend for all subplots\n",
        "for i, ax in enumerate(axs):\n",
        "    ax.set_xlabel('Time (s)')\n",
        "    ax.set_ylabel('Frequency (Hz)')\n",
        "    ax.grid(True)\n",
        "    ax.set_xlim(time_window)\n",
        "\n",
        "plt.savefig('plots/spec-time-frames.png')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0055c99346bb4ca48515739e3df95328": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b08c691f6a54ac6be07bf74dcbbd6e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d718a252d5714a9a8a83a368c79b7e9e",
            "placeholder": "​",
            "style": "IPY_MODEL_d48e31230164475fbb0e8b824a8fb873",
            "value": " Loss: 8.127e-01 | : 100%"
          }
        },
        "666de91617bd4128be4e5311d1ffd6e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7fb358e96b6047e7a7269bd2e4dbe698": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b08c691f6a54ac6be07bf74dcbbd6e3",
              "IPY_MODEL_9afd3c44061a46419320a9c061a2f7f3",
              "IPY_MODEL_8942cf0b80f14de1b4ff8abb949e7e06"
            ],
            "layout": "IPY_MODEL_0055c99346bb4ca48515739e3df95328"
          }
        },
        "8942cf0b80f14de1b4ff8abb949e7e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf78bf4ebb1e482d8ae7e40ba8bc2d9c",
            "placeholder": "​",
            "style": "IPY_MODEL_9c4b53b0744c4f4d92dfa37fe226d39f",
            "value": " 2500/2500 [01:01&lt;00:00, 41.39it/s]"
          }
        },
        "9afd3c44061a46419320a9c061a2f7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bab92db3dc8a43e3aa1f5f7c5d29b221",
            "max": 2500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_666de91617bd4128be4e5311d1ffd6e5",
            "value": 2500
          }
        },
        "9c4b53b0744c4f4d92dfa37fe226d39f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bab92db3dc8a43e3aa1f5f7c5d29b221": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf78bf4ebb1e482d8ae7e40ba8bc2d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d48e31230164475fbb0e8b824a8fb873": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d718a252d5714a9a8a83a368c79b7e9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
